# コンテナ × AI 推論環境 入門  
〜 LiteLLMで社内AIゲートウェイを構築する 〜

---

## 1. 今日のゴール

- AI推論環境の内部構造を理解する  
- コンテナ環境の構造理解  
- LiteLLMの役割を理解する  
- Redis / Postgresの用途を理解する  
- AIを社内インフラとして扱う視点を持つ  

---

## 2. 技術的なコンテナ環境の説明（重要）

### ● コンテナの特徴
- ホストOS（カーネル）を共有  
- プロセス分離（namespace）  
- リソース制御（cgroups）  
- 軽い、起動が速い、複製しやすい  

---

### ● VMとコンテナの違い（本質）

| 項目 | VM | コンテナ |
|---|---|---|
| OS | 仮想化 (ゲストOS) | 共有 (ホストOS) |
| 起動時間 | 数十秒〜数分 | 数百ms〜数秒 |
| CPU / メモリ | 多く消費 | 軽量 |
| 配布 | 重い | 軽量（MB〜GB） |
| 一貫性 | OSごと差異 | 依存関係が固定 |
| スケール | 台数単位 | コンテナ単位 |

---

### ● AI推論とコンテナが相性良い理由
- 依存ライブラリが重い（OpenAI / AWS / Azure SDK）  
- Python & Node の環境差異を排除  
- GPU & CPU の切り替えも簡単  
- 同一環境の複製が容易  
- モデルごとにコンテナ分離できる  

---

### ● 本番運用におけるコンテナの利点

- **Zero-Downtime デプロイ**
  - 新バージョン起動 → 古いの停止

- **スケールアウト**
  - 同じコンテナを増やすだけ

- **ロールバック**
  - 前のイメージに戻すだけ

- **依存関係の破壊防止**
  - “pip install して壊した” が発生しない

- **再現性**
  - ローカル ↔ 本番が一致  

---

### ● 今回のコンテナ構成例

