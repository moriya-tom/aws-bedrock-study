# コンテナ × AI 推論環境 入門  
〜 LiteLLMで社内AIゲートウェイを構築する 〜

---

## 1. 今日のゴール

- AI推論環境の内部構造を理解する  
- LiteLLMの役割を理解する  
- Redis / Postgresの用途を理解する  
- コンテナ化の意義を理解する  
- AIを社内インフラとして扱う視点を持つ  

---

## 2. 参加者前提（今回の想定）

- 全員エンジニア  
- AIは業務利用経験あり（ChatGPT / Copilotなど）  
- ただし推論環境の内部はブラックボックス  

---

## 3. 業務でのAI利用の現状

- 様々なモデルを使っている（例：GPT / Claude / Gemini）  
- 「APIを叩くだけ」の使い方になっている  
- 内部構造・通信・処理・課金の理解が弱い  

---

## 4. AIシステムの課題（現実）

- モデルごとにSDK/API形式が違う  
- 使用ログが残らない  
- コスト分析ができない  
- キャッシュが効かず課金が増える  
- 権限管理ができない  
- 複数モデルの横断運用が困難  
- 「モデル乗り換えコスト」が高い  

---

## 5. 解決：AIゲートウェイを自前で構築する

メリット：

- モデルAPIの統一  
- モデル切替が容易  
- 使用ログを完全記録  
- キャッシュによる高速化  
- 課金管理が可能  
- 社内統制・ガバナンスが強化  
- セキュアな内部AI基盤  

---

## 6. システム構成（役割）

| コンポーネント  | 役割 |
|---|---|
| LiteLLM | AIゲートウェイ / API統一 |
| Redis | キャッシュ / レート制御 |
| PostgreSQL | 使用ログ / メトリクス / 課金情報 |
| Azure / AWS / 各AIモデル | AI推論の実行 |

---

## 7. コンテナ化する理由

- 環境依存を排除  
- ライブラリ破壊が起きない  
- 再現性のある実行環境  
- 複数インスタンスでスケール  
- 障害切り分けが容易  
- セルフサービス運用可能  

---

# 🔧 デモ編（Azure VM上で）

## 8. LiteLLM を起動

